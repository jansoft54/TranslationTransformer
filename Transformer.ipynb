{"cells":[{"cell_type":"code","execution_count":78,"metadata":{"executionInfo":{"elapsed":322,"status":"ok","timestamp":1728663711019,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"XWfg66Sbi3_7"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n"]},{"cell_type":"code","execution_count":79,"metadata":{"executionInfo":{"elapsed":356,"status":"ok","timestamp":1728665086458,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"GPN_U7XbcjC3"},"outputs":[],"source":["class InputEmbedding(nn.Module):\n","    def __init__(self, vocab_size, d_model):\n","        super().__init__()\n","        self.d_model = d_model\n","        self.vocab_size = vocab_size\n","        self.embedding = nn.Embedding(vocab_size, d_model).to('cuda')\n","\n","    def forward(self, x):\n","        return self.embedding(x) * self.d_model ** 0.5\n"]},{"cell_type":"code","execution_count":80,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728665086788,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"Dd48LAQxjLf0"},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","  def __init__(self,d_model,seq_len,drop_out=0.1):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.seq_len = seq_len\n","    self.droup_out = nn.Dropout(drop_out)\n","    pe = torch.zeros(seq_len,d_model)\n","    pos = torch.arange(0,seq_len,dtype=torch.float).unsqueeze(1)\n","    denom = torch.exp(torch.arange(0,d_model,2).float()*(-torch.log(torch.tensor(10000.0))/d_model))\n","    pe[:,0::2] = torch.sin(pos/denom)\n","    pe[:,1::2] = torch.cos(pos/denom)\n","    pe = pe.unsqueeze(0).to('cuda')\n","    self.register_buffer('pe',pe)\n","\n","  def forward(self,x):\n","    x = x + (self.pe[:,:x.shape[1],:]).requires_grad_(False)\n","    return self.droup_out(x)\n","\n","\n"]},{"cell_type":"code","execution_count":81,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728665087863,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"d1Hz5WS7sHBA"},"outputs":[],"source":["class LayerNorm(nn.Module):\n","  def __init__(self,ndim,eps=10**-6):\n","    super().__init__()\n","    self.eps = eps\n","    self.alpha = nn.Parameter(torch.ones(ndim)).to('cuda')\n","    self.beta = nn.Parameter(torch.zeros(ndim)).to('cuda')\n","\n","  def forward(self,x):\n","    mean = x.mean(-1,keepdim=True) # if the first dim is the batch dim\n","    std = x.std(-1,keepdim=True)\n","    return self.alpha*(x-mean)/(std+self.eps) + self.beta"]},{"cell_type":"code","execution_count":82,"metadata":{"executionInfo":{"elapsed":337,"status":"ok","timestamp":1728665089630,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"tTcCwsavswVp"},"outputs":[],"source":["class MLP(nn.Module):\n","  def __init__(self,d_model,ff_dim=2048,drop_out=0.1):\n","    super().__init__()\n","    self.layer1 = nn.Linear(d_model,ff_dim).to('cuda')\n","    self.layer2 = nn.Linear(ff_dim,d_model).to('cuda')\n","    self.drop_put = nn.Dropout(drop_out).to('cuda')\n","    self.gelu = nn.GELU().to('cuda')\n","\n","  def forward(self,x):\n","    x = self.layer1(x)\n","    x = self.gelu(x)\n","    x = self.layer2(x)\n","    x = self.drop_put(x)\n","    return x"]},{"cell_type":"code","execution_count":83,"metadata":{"executionInfo":{"elapsed":302,"status":"ok","timestamp":1728665093068,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"WesnIEHrt3WC"},"outputs":[],"source":["import math\n","class MultiHeadAttentionBlock(nn.Module):\n","  def __init__(self,d_model,seq_len,n_heads=6,drop_out=0.1):\n","    super().__init__()\n","    assert d_model % n_heads == 0\n","    self.d_model = d_model\n","    self.seq_len = seq_len\n","    self.n_heads = n_heads\n","    self.drop_out = nn.Dropout(drop_out)\n","    self.head_dim = d_model//n_heads\n","    self.w_q = nn.Linear(self.d_model, self.d_model).to('cuda')\n","    self.w_k = nn.Linear(self.d_model, self.d_model).to('cuda')\n","    self.w_v = nn.Linear(self.d_model, self.d_model).to('cuda')\n","    self.w_o = nn.Linear(self.d_model, self.d_model).to('cuda')\n","\n","  def attention(self,q,k,v,mask):\n","    d_k = (q.shape[-1])\n","    mask = mask.unsqueeze(1).expand(-1, q.shape[1], -1, -1)\n","    scores = (q@k.transpose(-2,-1))/math.sqrt(d_k)\n","    assert mask != None\n","    scores = scores.masked_fill(mask==0,float(\"-inf\"))\n","    scores = scores.softmax(dim=-1)\n","    scores = self.drop_out(scores)\n","\n","    return torch.matmul(scores,v),scores\n","\n","\n","  def forward(self,q,k,v,mask):\n","    batch_size = q.shape[0]\n","\n","    q,k,v = self.w_q(q),self.w_k(k),self.w_v(v)\n","\n","    q = q.reshape(batch_size,self.seq_len,self.n_heads,self.head_dim)\n","    k = k.reshape(batch_size,self.seq_len,self.n_heads,self.head_dim)\n","    v = v.reshape(batch_size,self.seq_len,self.n_heads,self.head_dim)\n","    q,k,v = q.transpose(1,2),k.transpose(1,2),v.transpose(1,2) # batch,seq_len,n_heads,d_k ->  batch,n_heads,seq_len,d_k\n","    x,attention_scores = self.attention(q,k,v,mask)\n","    x = x.transpose(1,2).contiguous().view(batch_size,self.seq_len,self.d_model) #  batch,n_heads,seq_len,d_k ->  batch,seq_len,n_heads,d_k -> batch,seq_len,d_model\n","    return self.w_o(x)\n"]},{"cell_type":"code","execution_count":84,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1728665094146,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"DZp21hQ-u2Bw"},"outputs":[],"source":["class ResBlock(nn.Module):\n","  def __init__(self,d_model,seq_len,drop_out=0.1):\n","    super().__init__()\n","    self.norm = LayerNorm(ndim=d_model)\n","    self.drop_out = nn.Dropout(drop_out)\n","\n","  def forward(self,x,sub):\n","    return x + self.drop_out(sub(self.norm(x)))"]},{"cell_type":"code","execution_count":85,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728665095048,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"fbt5Lk1_8YlQ"},"outputs":[],"source":["class EncoderBlock(nn.Module):\n","  def __init__(self,d_model,seq_len,n_heads=6,ff_dim=2048,drop_out=0.1):\n","    super().__init__()\n","    self.mha = MultiHeadAttentionBlock(d_model=d_model,seq_len=seq_len,n_heads=n_heads,drop_out=drop_out)\n","    self.mlp = MLP(d_model=d_model,ff_dim=ff_dim,drop_out=drop_out)\n","    self.res = nn.ModuleList([ResBlock(d_model,seq_len,drop_out) for _ in range(2)])\n","\n","  def forward(self,x,src_mask):\n","    x = self.res[0](x,lambda x:self.mha(x,x,x,src_mask))\n","    x = self.res[1](x,self.mlp)\n","    return x\n","\n","class Encoder(nn.Module):\n","  def __init__(self,d_model,module_list):\n","    super().__init__()\n","    self.module_list = module_list\n","    self.norm = LayerNorm(ndim=d_model)\n","\n","  def forward(self,x,src_mask):\n","    for module in self.module_list:\n","      x = module(x,src_mask)\n","    return self.norm(x)"]},{"cell_type":"code","execution_count":73,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728665096194,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"ioTm5hCc-TqS"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":86,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728665097071,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"js2yrmXe-t_w"},"outputs":[],"source":["class DecoderBlock(nn.Module):\n","  def __init__(self,d_model,seq_len,n_heads=6,ff_dim=512,drop_out=0.1):\n","    super().__init__()\n","    self.self_attention = MultiHeadAttentionBlock(d_model=d_model,seq_len=seq_len,n_heads=n_heads,drop_out=drop_out)\n","    self.cross_attention = MultiHeadAttentionBlock(d_model=d_model,seq_len=seq_len,n_heads=n_heads,drop_out=drop_out)\n","    self.mlp = MLP(d_model=d_model,ff_dim=ff_dim,drop_out=drop_out)\n","    self.layer_norm1 = LayerNorm(ndim=d_model)\n","    self.layer_norm2 = LayerNorm(ndim=d_model)\n","\n","  def forward(self,x,encoder_output,src_mask,tgt_mask):\n","    x_n = self.layer_norm1(x)\n","    x = x + self.self_attention(x_n,x_n,x_n,tgt_mask)\n","    x = x + self.mlp(self.layer_norm2(x))\n","    return x\n","\n","class Decoder(nn.Module):\n","  def __init__(self,d_model,module_list):\n","    super().__init__()\n","    self.module_list = nn.ModuleList(module_list)\n","    self.norm = LayerNorm(ndim=d_model)\n","\n","  def forward(self,x,encoder_output,src_mask,tgt_mask):\n","    for layer in self.module_list:\n","      x = layer(x,encoder_output,src_mask,tgt_mask)\n","\n","    return self.norm(x)"]},{"cell_type":"code","execution_count":87,"metadata":{"executionInfo":{"elapsed":340,"status":"ok","timestamp":1728665098404,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"1XY7RDeKAY4B"},"outputs":[],"source":["class Classification(nn.Module):\n","  def __init__(self,d_model,vocab_size):\n","    super().__init__()\n","    self.layer = nn.Linear(d_model,vocab_size)\n","\n","  def forward(self,x):\n","    return F.log_softmax(self.layer(x),dim=-1)\n"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728665099247,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"OLA8a15gBShO"},"outputs":[],"source":["class Transformer(nn.Module):\n","  def __init__(self,vocab_size,seq_len,ignore_index=-100,d_model=256,n_heads=4,ff_dim=512,drop_out=0.1):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.seq_len = seq_len\n","    self.vocab_size = vocab_size\n","    self.input_embedding = InputEmbedding(vocab_size,d_model)\n","    self.output_embedding = InputEmbedding(vocab_size,d_model)\n","    self.positional_encoding_src = PositionalEncoding(d_model,seq_len,drop_out)\n","    self.positional_encoding_tgt = PositionalEncoding(d_model,seq_len,drop_out)\n","\n","    self.encoder = Encoder(module_list=[EncoderBlock(d_model,seq_len,n_heads,ff_dim,drop_out) for _ in range(6)],\n","                           d_model=d_model)\n","    self.decoder = Decoder(module_list=[DecoderBlock(d_model,seq_len,n_heads,ff_dim,drop_out) for _ in range(6)],\n","                           d_model=d_model)\n","    self.classification = Classification(d_model,vocab_size)\n","    self.cross_entropy = nn.CrossEntropyLoss(ignore_index=ignore_index)\n","\n","  def encode(self,src,src_mask):\n","    src = self.input_embedding(src)\n","    src = self.positional_encoding_src(src)\n","    return self.encoder(src,src_mask)\n","\n","  def decode(self,tgt,encoder_output,src_mask,tgt_mask):\n","    tgt = self.output_embedding(tgt)\n","    tgt = self.positional_encoding_tgt(tgt)\n","    return self.decoder(tgt,encoder_output,src_mask,tgt_mask)\n","\n","  def classify(self,x):\n","    return self.classification(x)\n","\n","  def forward(self,src,tgt,src_mask,tgt_mask):\n","    #decoder only\n","    decoder_output = self.decode(src,src,src_mask,src_mask)\n","    logits = self.classify(decoder_output)\n","    loss = self.cross_entropy(logits.view(-1, logits.size(-1)),tgt.view(-1))\n","    return logits,loss\n","\n"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1728663875507,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"yvpPuXn98-IF"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10700,"status":"ok","timestamp":1728665116063,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"Q5oFhcdgB2pY","outputId":"08c565a6-bd4d-4f66-da70-30ef73d01d45"},"outputs":[{"name":"stdout","output_type":"stream","text":["65\n"]}],"source":["import numpy as np\n","from transformers import BartTokenizer\n","\n","tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n","# Read the text data from the file\n","with open('./trump_3.6.txt', 'r', encoding='utf-8') as file:\n","    text = file.read()\n","\n","lines = text.split(\".\")\n","lines = [lines[i]+\". \" + lines[i + 1]+\".\" for i in range(0, len(lines) - 1, 2)]\n","\n","max_length = 65\n","# Tokenize each line and extract input_ids, pad/truncate to max_length\n","encoded_lines = [\n","    tokenizer.encode(\n","        line.strip(),\n","        add_special_tokens=True,\n","        padding='max_length',\n","        max_length=max_length,\n","        truncation=True\n","    )\n","    for line in lines if line.strip()\n","]\n","print(len(encoded_lines[0]))\n","# Filter out any lines that may have resulted in empty tokens\n","#encoded_lines = torch.Tensor(encoded_lines)\n"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":331,"status":"ok","timestamp":1728665124873,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"sreqPWRQsKyk","outputId":"8c42a37a-193a-4b3f-da13-7532a5ae703d"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([    0,  2387,  2598,  1791,     6,    38,   236,     7,  1994,     7,\n","           47,  3422,    59,     5, 15554,  1061,     9,     5,   375,   186,\n","            4,  1437,   287,    38,    33,    26,     6,     5,  5853, 29471,\n","            9,     5,   382,  6107,  2322,    23,     5,   182,  1144,     9,\n","           84,  3497,     4,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1], device='cuda:0') tensor([ 2387,  2598,  1791,     6,    38,   236,     7,  1994,     7,    47,\n","         3422,    59,     5, 15554,  1061,     9,     5,   375,   186,     4,\n","         1437,   287,    38,    33,    26,     6,     5,  5853, 29471,     9,\n","            5,   382,  6107,  2322,    23,     5,   182,  1144,     9,    84,\n","         3497,     4,     2,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n","            1,     1,     1,     1], device='cuda:0')\n"]}],"source":["input_tensor = torch.tensor(encoded_lines)\n","\n","# Remove the </s> token from the src sequences\n","# We will identify the </s> token ID from the tokenizer\n","eos_token_id = tokenizer.eos_token_id\n","pad_token_id = tokenizer.pad_token_id\n","\n","seq_len=64\n","# Remove the last token from src if it is the </s> token\n","src = torch.where(input_tensor[:, :-1] == eos_token_id, tokenizer.pad_token_id, input_tensor[:, :-1]).to('cuda')\n","targets = input_tensor[:, 1:].to('cuda')  # Shifted target, all tokens except the first one\n","tril_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool))\n","tril_mask=tril_mask.unsqueeze(0).expand(src.size(0), -1, -1).to('cuda')\n","\n","print(src[0],targets[0])"]},{"cell_type":"code","execution_count":95,"metadata":{"executionInfo":{"elapsed":329,"status":"ok","timestamp":1728665126504,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"gaO29EI3a32T"},"outputs":[],"source":["def save(epoch,model,optim,loss):\n","  checkpoint_path = 'model_checkpoint.pth'\n","  torch.save({\n","      'epoch': epoch,\n","      'model_state_dict': model.state_dict(),\n","      'optimizer_state_dict': optim.state_dict(),\n","      'loss': loss,\n","  }, checkpoint_path)"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["from torch.optim.lr_scheduler import _LRScheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","\n","class GradualWarmupScheduler(_LRScheduler):\n","    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n","    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n","\n","    Args:\n","        optimizer (Optimizer): Wrapped optimizer.\n","        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n","        total_epoch: target learning rate is reached at total_epoch, gradually\n","        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n","    \"\"\"\n","\n","    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n","        self.multiplier = multiplier\n","        if self.multiplier < 1.:\n","            raise ValueError('multiplier should be greater thant or equal to 1.')\n","        self.total_epoch = total_epoch\n","        self.after_scheduler = after_scheduler\n","        self.finished = False\n","        super(GradualWarmupScheduler, self).__init__(optimizer)\n","\n","    def get_lr(self):\n","        if self.last_epoch > self.total_epoch:\n","            if self.after_scheduler:\n","                if not self.finished:\n","                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n","                    self.finished = True\n","                return self.after_scheduler.get_last_lr()\n","            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n","\n","        if self.multiplier == 1.0:\n","            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","\n","    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n","        if epoch is None:\n","            epoch = self.last_epoch + 1\n","        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n","        if self.last_epoch <= self.total_epoch:\n","            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n","                param_group['lr'] = lr\n","        else:\n","            if epoch is None:\n","                self.after_scheduler.step(metrics, None)\n","            else:\n","                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n","\n","    def step(self, epoch=None, metrics=None):\n","        if type(self.after_scheduler) != ReduceLROnPlateau:\n","            if self.finished and self.after_scheduler:\n","                if epoch is None:\n","                    self.after_scheduler.step(None)\n","                else:\n","                    self.after_scheduler.step(epoch - self.total_epoch)\n","                self.last_epoch = self.after_scheduler.last_epoch + self.total_epoch + 1\n","                self._last_lr = self.after_scheduler.get_last_lr()\n","            else:\n","                return super(GradualWarmupScheduler, self).step(epoch)\n","        else:\n","            self.step_ReduceLROnPlateau(metrics, epoch)"]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1067380,"status":"ok","timestamp":1728666194681,"user":{"displayName":"Hdt Soft","userId":"01818080135613168476"},"user_tz":-120},"id":"DXLiVwaZu6n0","outputId":"be4ae3fd-0d16-4f43-f4a6-4c68c79bc7b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Params 43389273\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[108], line 29\u001b[0m\n\u001b[1;32m     24\u001b[0m batch_src, batch_targets,batch_mask \u001b[38;5;241m=\u001b[39m batch_src\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m), batch_targets\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m),batch_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     25\u001b[0m logits,loss \u001b[38;5;241m=\u001b[39m model(src\u001b[38;5;241m=\u001b[39mbatch_src,\n\u001b[1;32m     26\u001b[0m       tgt\u001b[38;5;241m=\u001b[39mbatch_targets,\n\u001b[1;32m     27\u001b[0m       src_mask\u001b[38;5;241m=\u001b[39mbatch_mask,\n\u001b[1;32m     28\u001b[0m       tgt_mask\u001b[38;5;241m=\u001b[39mbatch_mask)\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from torch.utils.data import TensorDataset, DataLoader\n","\n","dataset = TensorDataset(src, targets,tril_mask)\n","batch_size = 64  # Adjust based on your memory capacity and requirements\n","\n","# Create the DataLoader\n","data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","model = Transformer(vocab_size=tokenizer.vocab_size,\n","                    ignore_index=pad_token_id,\n","                    seq_len=seq_len)\n","total_params = sum(p.numel() for p in model.parameters())\n","print(\"Total Params\",total_params)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.00002,betas=(0.9, 0.98), eps=1e-9,weight_decay=0.001)\n","model.to('cuda:0')\n","model.train()\n","epochs = 50\n","lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,eta_min=1e-8,T_max=epochs)\n","scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=epochs, after_scheduler=lr_scheduler)\n","\n","for epoch in range(epochs):\n","  for batch in data_loader:\n","      batch_src, batch_targets,batch_mask = batch\n","      batch_src, batch_targets,batch_mask = batch_src.to('cuda'), batch_targets.to('cuda'),batch_mask.to('cuda')\n","      logits,loss = model(src=batch_src,\n","            tgt=batch_targets,\n","            src_mask=batch_mask,\n","            tgt_mask=batch_mask)\n","      loss.backward()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","\n","      optimizer.step()\n","      optimizer.zero_grad()\n","  save(epoch,model,optimizer,loss)\n","  lr_scheduler.step()\n","  print(\"loss\",loss)\n","\n","  #  print(\"Source batch shape:\", batch_src[0])\n","   # print(\"Target batch shape:\", batch_targets[0])\n","    #print(\"Mask batch shape:\", batch_mask[0])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xBOrsaO8UrL"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMGfa25WSsJ0giZCyaZ91Mb","gpuType":"T4","mount_file_id":"11L7TQpjuLaQnsyXi6UP_3cXpPaXfQFEs","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
